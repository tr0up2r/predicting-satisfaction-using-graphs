{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de58965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f25427",
   "metadata": {},
   "source": [
    "# BertForSequenceClassification Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d8aae",
   "metadata": {},
   "source": [
    "## preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee067fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_post.csv', encoding='UTF-8')\n",
    "comment_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_comment.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83bcf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "# texts (x)\n",
    "post_contents = list(post_df['content'])\n",
    "comment_bodies = list(comment_df['content'])\n",
    "\n",
    "# satisfaction score (y)\n",
    "satisfactions_float = list(post_df['satisfaction'])\n",
    "satisfactions = []\n",
    "\n",
    "for s in satisfactions_float:\n",
    "    if s < 3.5:\n",
    "        satisfactions.append(0)\n",
    "    elif s < 5:\n",
    "        satisfactions.append(1)\n",
    "    else:\n",
    "        satisfactions.append(2)\n",
    "\n",
    "data = []\n",
    "\n",
    "for content, body, satisfaction in zip(post_contents, comment_bodies, satisfactions):\n",
    "    data.append([content + '[SEP]' + body, satisfaction])\n",
    "\n",
    "columns = ['contents', 'label']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# data split (train & test sets)\n",
    "idx_train, idx_remain = train_test_split(df.index.values, test_size=0.20, random_state=42)\n",
    "idx_val, idx_test = train_test_split(idx_remain, test_size=0.50, random_state=42)\n",
    "\n",
    "print(idx_train.shape, idx_val.shape, idx_test.shape)\n",
    "\n",
    "train_df = df.iloc[idx_train]\n",
    "val_df = df.iloc[idx_val]\n",
    "test_df = df.iloc[idx_test]\n",
    "\n",
    "count_min_label = min(train_df['label'].value_counts())\n",
    "\n",
    "labels = [0, 1, 2]\n",
    "\n",
    "train_sample_df = pd.DataFrame([], columns=columns)\n",
    "\n",
    "for label in labels:\n",
    "    tmp = train_df[train_df['label'] == label]\n",
    "    tmp_sampled = tmp.sample(frac=1).iloc[:count_min_label]\n",
    "    train_sample_df = pd.concat([train_sample_df, tmp_sampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1b60d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>For some reason watching a horrifying movie &amp;g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>My depression/anxiety has been back in full sw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>I had to be the ugly one.I had to be the depre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>I would take a bullet for her. I would go as f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>There's no one I want to be around.  I spend t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Iâm about three weeks into my antidepressant...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Or a meeting place where you don't have to soc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>I love putting on some head phones and blaring...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>I was a 21 year old student studying physics a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>I cut again. After being 3 fucking months clea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              contents label\n",
       "880  For some reason watching a horrifying movie &g...     0\n",
       "514  My depression/anxiety has been back in full sw...     0\n",
       "287  I had to be the ugly one.I had to be the depre...     0\n",
       "505  I would take a bullet for her. I would go as f...     0\n",
       "435  There's no one I want to be around.  I spend t...     0\n",
       "..                                                 ...   ...\n",
       "596  Iâm about three weeks into my antidepressant...     2\n",
       "154  Or a meeting place where you don't have to soc...     2\n",
       "266  I love putting on some head phones and blaring...     2\n",
       "111  I was a 21 year old student studying physics a...     2\n",
       "633  I cut again. After being 3 fucking months clea...     2\n",
       "\n",
       "[546 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e7143",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe83e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f04f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = f'../predicting-satisfaction-using-graphs/model/epoch_10.model'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3, problem_type='multi_label_classification', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0883a3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3e76ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70778ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d7aac52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2353,  2095,  2267,  3076,  1012,  2026,  6245,  2038,  2467,\n",
       "         27674,  4321,  2013,  2591, 10089,  1010, 10261,  1045,  2018,  2814,\n",
       "          2021,  2025,  4209,  2129,  2000,  2191,  2068,  1012,  1045,  2985,\n",
       "          2048,  2086,  1999, 19568,  2015,  1998,  2196,  3764,  2000,  2151,\n",
       "          1997,  2026, 10638,  1998,  2134,  1005,  1056,  2428,  3113, 10334,\n",
       "          1012,  2197,  2305,  1010,  2005,  1996,  2034,  2051,  1999,  2026,\n",
       "          2166,  1010,  1045,  2001,  4778,  2000,  1037,  2283,  1006,  2025,\n",
       "          7714,  2568,  2017,  1025,  1045,  2001,  1037,  2112,  1997,  1037,\n",
       "          2177,  2008,  2288, 13643,  4778,  1007,  1012,  1045, 10749,  1037,\n",
       "          2210,  2978,  1010,  2021,  2288, 10247,  2100,  2855,  1010,  1998,\n",
       "          2025,  4209,  2026,  6537,  1045,  3030,  1012,  1045,  2001,  3294,\n",
       "         17358,  2306,  1996,  2034,  3178,  1012,  4661, 25795,  1998,  2019,\n",
       "          8138, 14806,  1010,  1996,  2069,  2111,  1045,  2941,  3764,  2000,\n",
       "          2020,  1996,  2111,  1045,  2525,  2354,  1012,  1045,  2059,  2985,\n",
       "          1037,  2261,  2847,  3564,  1999,  1037,  3420,  1998, 14158,  7144,\n",
       "         27440,  1012,  2009,  2001,  3492,  6881,  1010, 14158,  2048,  2111,\n",
       "          2175,  2013,  3143, 12358,  1010,  2000,  2108,  1999,  2028,  2178,\n",
       "          1005,  1055,  2608,  2012,  1996,  2203,  1997,  1996,  2305,  1012,\n",
       "          3504,  2061,  3733,  1012,  2003,  2023,  1996,  4485,  2008,  1045,\n",
       "          1005,  2310,  2042,  1000,  4394,  1000,  2005,  2035,  1997,  2026,\n",
       "          2166,  1029,  1999,  2029,  2553,  1010, 10166,  1012,  2204,  9436,\n",
       "         25514,  1010,  1045,  1005,  2222,  2202,  2026,  6245,  2058,  2008,\n",
       "          2151,  2154,  1012,  3374,  2055,  1996,  2743,  2102,  1010,  1045,\n",
       "          3984,  2008,  1005,  1055,  2035,  2009,  2428,  2003,  1012,  1045,\n",
       "          2507,  2039,  1012,   102,  2028,  2518,  1045,  2572,  2183,  2000,\n",
       "          4012,  3549,  2094,  2017,  2006,  2025, 11664,  2135,  5948,  1998,\n",
       "          4760, 19355,  1012,  5948,  2003,  2788,  3378,  2007,  2591,  3450,\n",
       "          2021,  1996,  3119,  7245,  1997,  2108,  1037,  2990, 12054,  2003,\n",
       "          2025,  4276,  2009,  1012,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(data[0][0], max_length=512, padding='max_length', truncation=True,return_tensors='pt')\n",
    "inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124e906",
   "metadata": {},
   "source": [
    "## Custom forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48fd9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    output = model(**inputs)\n",
    "    return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69523e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_forward_func(inputs):\n",
    "    pred = predict(inputs)\n",
    "    pred = np.argmax(pred.cpu().detach().numpy(), axis=1)\n",
    "    \n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "711be324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classification_forward_func(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c34dfe",
   "metadata": {},
   "source": [
    "## Define baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8869c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference (zero padding)\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between post and comment and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated post-comment word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e0c8697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 102 101\n"
     ]
    }
   ],
   "source": [
    "print(ref_token_id, sep_token_id, cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fb01ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(post, comment, ref_token_id, sep_token_id, cls_token_id):\n",
    "    post_ids = tokenizer.encode(post, add_special_tokens=False)\n",
    "    comment_ids = tokenizer.encode(comment, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = torch.tensor([[cls_token_id] + post_ids + [sep_token_id] + comment_ids + [sep_token_id]], device=device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    \n",
    "    # token_type_ids is used both embeddings\n",
    "    token_type_ids = torch.tensor([[0 for i in range(seq_len)]], device=device)\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(post_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(comment_ids) + [sep_token_id]\n",
    "\n",
    "    return input_ids, torch.tensor([ref_input_ids], device=device), token_type_ids, len(post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cd7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b919396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101,  2353,  2095,  2267,  3076,  1012,  2026,  6245,  2038,  2467,\n",
      "         27674,  4321,  2013,  2591, 10089,  1010, 10261,  1045,  2018,  2814,\n",
      "          2021,  2025,  4209,  2129,  2000,  2191,  2068,  1012,  1045,  2985,\n",
      "          2048,  2086,  1999, 19568,  2015,  1998,  2196,  3764,  2000,  2151,\n",
      "          1997,  2026, 10638,  1998,  2134,  1005,  1056,  2428,  3113, 10334,\n",
      "          1012,  2197,  2305,  1010,  2005,  1996,  2034,  2051,  1999,  2026,\n",
      "          2166,  1010,  1045,  2001,  4778,  2000,  1037,  2283,  1006,  2025,\n",
      "          7714,  2568,  2017,  1025,  1045,  2001,  1037,  2112,  1997,  1037,\n",
      "          2177,  2008,  2288, 13643,  4778,  1007,  1012,  1045, 10749,  1037,\n",
      "          2210,  2978,  1010,  2021,  2288, 10247,  2100,  2855,  1010,  1998,\n",
      "          2025,  4209,  2026,  6537,  1045,  3030,  1012,  1045,  2001,  3294,\n",
      "         17358,  2306,  1996,  2034,  3178,  1012,  4661, 25795,  1998,  2019,\n",
      "          8138, 14806,  1010,  1996,  2069,  2111,  1045,  2941,  3764,  2000,\n",
      "          2020,  1996,  2111,  1045,  2525,  2354,  1012,  1045,  2059,  2985,\n",
      "          1037,  2261,  2847,  3564,  1999,  1037,  3420,  1998, 14158,  7144,\n",
      "         27440,  1012,  2009,  2001,  3492,  6881,  1010, 14158,  2048,  2111,\n",
      "          2175,  2013,  3143, 12358,  1010,  2000,  2108,  1999,  2028,  2178,\n",
      "          1005,  1055,  2608,  2012,  1996,  2203,  1997,  1996,  2305,  1012,\n",
      "          3504,  2061,  3733,  1012,  2003,  2023,  1996,  4485,  2008,  1045,\n",
      "          1005,  2310,  2042,  1000,  4394,  1000,  2005,  2035,  1997,  2026,\n",
      "          2166,  1029,  1999,  2029,  2553,  1010, 10166,  1012,  2204,  9436,\n",
      "         25514,  1010,  1045,  1005,  2222,  2202,  2026,  6245,  2058,  2008,\n",
      "          2151,  2154,  1012,  3374,  2055,  1996,  2743,  2102,  1010,  1045,\n",
      "          3984,  2008,  1005,  1055,  2035,  2009,  2428,  2003,  1012,  1045,\n",
      "          2507,  2039,  1012,   102,  2028,  2518,  1045,  2572,  2183,  2000,\n",
      "          4012,  3549,  2094,  2017,  2006,  2025, 11664,  2135,  5948,  1998,\n",
      "          4760, 19355,  1012,  5948,  2003,  2788,  3378,  2007,  2591,  3450,\n",
      "          2021,  1996,  3119,  7245,  1997,  2108,  1037,  2990, 12054,  2003,\n",
      "          2025,  4276,  2009,  1012,   102]]), tensor([[101,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0, 102]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 242)\n"
     ]
    }
   ],
   "source": [
    "print(construct_input_ref_pair(post_contents[0], comment_bodies[0], ref_token_id, sep_token_id, cls_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9db0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9077839",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396a3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de5290e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2771,  0.6628,  0.6646,  ...,  0.4570, -0.3214,  0.4301],\n",
      "         [-0.6448,  0.5613,  1.9034,  ...,  1.4501,  0.6793, -0.8462],\n",
      "         [-0.3622,  1.1128,  1.3065,  ...,  0.9648,  0.6278, -0.7356],\n",
      "         ...,\n",
      "         [-0.1370,  1.3976,  0.9012,  ...,  1.1594, -0.1056, -0.3206],\n",
      "         [ 0.1244,  1.1471,  0.3618,  ...,  1.0564, -0.4413, -0.4324],\n",
      "         [-0.1614,  0.8678,  0.9781,  ...,  0.9212, -0.3137, -0.6701]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63e66200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08ce640a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_func을 뭔가 바꿔야 할 듯. baseline 필요!! 이게 아마 0 vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ca97129",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(model, model.bert.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e6419d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2515792/3238897907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    492\u001b[0m         )\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         attributions = self.ig.attribute.__wrapped__(  # type: ignore\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mig\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0minputs_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             attributions = self._attribute(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         grads = self.gradient_func(\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mforward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mgradient_func\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    468\u001b[0m                         \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                     output = _run_forward(\n\u001b[0m\u001b[1;32m    471\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                     )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_select_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_select_targets\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m     \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "attribution = lig.attribute(inputs.input_ids, target=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd897ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b903482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding = 0\n",
    "ref_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9650ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(ref_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e78cf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a870ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec355e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
