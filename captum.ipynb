{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "de58965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f25427",
   "metadata": {},
   "source": [
    "# BertForSequenceClassification Captum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d8aae",
   "metadata": {},
   "source": [
    "## preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4ee067fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_post.csv', encoding='UTF-8')\n",
    "comment_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_comment.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a83bcf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) (100,) (100,)\n"
     ]
    }
   ],
   "source": [
    "# texts (x)\n",
    "post_contents = list(post_df['content'])\n",
    "comment_bodies = list(comment_df['content'])\n",
    "\n",
    "# satisfaction score (y)\n",
    "satisfactions_float = list(post_df['satisfaction'])\n",
    "satisfactions = []\n",
    "\n",
    "for s in satisfactions_float:\n",
    "    if s < 3.5:\n",
    "        satisfactions.append(0)\n",
    "    elif s < 5:\n",
    "        satisfactions.append(1)\n",
    "    else:\n",
    "        satisfactions.append(2)\n",
    "\n",
    "data = []\n",
    "\n",
    "for content, body, satisfaction in zip(post_contents, comment_bodies, satisfactions):\n",
    "    data.append([content + '[SEP]' + body, satisfaction])\n",
    "\n",
    "columns = ['contents', 'label']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# data split (train & test sets)\n",
    "idx_train, idx_remain = train_test_split(df.index.values, test_size=0.20, random_state=42)\n",
    "idx_val, idx_test = train_test_split(idx_remain, test_size=0.50, random_state=42)\n",
    "\n",
    "print(idx_train.shape, idx_val.shape, idx_test.shape)\n",
    "\n",
    "train_df = df.iloc[idx_train]\n",
    "val_df = df.iloc[idx_val]\n",
    "test_df = df.iloc[idx_test]\n",
    "\n",
    "count_min_label = min(train_df['label'].value_counts())\n",
    "\n",
    "labels = [0, 1, 2]\n",
    "\n",
    "train_sample_df = pd.DataFrame([], columns=columns)\n",
    "\n",
    "for label in labels:\n",
    "    tmp = train_df[train_df['label'] == label]\n",
    "    tmp_sampled = tmp.sample(frac=1).iloc[:count_min_label]\n",
    "    train_sample_df = pd.concat([train_sample_df, tmp_sampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344095e6",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "fe83e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "0f04f2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = f'../predicting-satisfaction-using-graphs/model/epoch_10.model'\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3, problem_type='multi_label_classification', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0883a3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f3e76ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "70778ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d20d15",
   "metadata": {},
   "source": [
    "## Custom forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "eeadb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, attention_mask=None):\n",
    "    inputs = {'input_ids': inputs,\n",
    "             'token_type_ids': token_type_ids,\n",
    "             'attention_mask': attention_mask}\n",
    "    output = model(**inputs)\n",
    "        \n",
    "    return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6fcc5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_forward_func(inputs, token_type_ids=None, attention_mask=None):\n",
    "    pred = predict(inputs, token_type_ids, attention_mask)\n",
    "    # pred = np.argmax(pred.cpu().detach().numpy(), axis=1)\n",
    "    # return torch.tensor(pred, device=device)\n",
    "    # return pred\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4f25c6",
   "metadata": {},
   "source": [
    "## Define baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1f01d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference (zero padding)\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between post and comment and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated post-comment word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "95105194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 102 101\n"
     ]
    }
   ],
   "source": [
    "print(ref_token_id, sep_token_id, cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0f6573a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(post, comment, ref_token_id, sep_token_id, cls_token_id):\n",
    "    post_ids = tokenizer.encode(post, add_special_tokens=False)\n",
    "    comment_ids = tokenizer.encode(comment, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = torch.tensor([[cls_token_id] + post_ids + [sep_token_id] + comment_ids + [sep_token_id]], device=device)\n",
    "    seq_len = input_ids.size(1)\n",
    "    \n",
    "    # token_type_ids is used both embeddings\n",
    "    token_type_ids = torch.tensor([[0 for i in range(seq_len)]], device=device)\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = torch.tensor([[cls_token_id] + [ref_token_id] * len(post_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(comment_ids) + [sep_token_id]], device=device)\n",
    "\n",
    "    return input_ids, ref_input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c17c9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2fefa0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, token_type_ids):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=token_type_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "f3af30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "post, comment = post_contents[0], comment_bodies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "739bf034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101,  2353,  2095,  2267,  3076,  1012,  2026,  6245,  2038,  2467,\n",
      "         27674,  4321,  2013,  2591, 10089,  1010, 10261,  1045,  2018,  2814,\n",
      "          2021,  2025,  4209,  2129,  2000,  2191,  2068,  1012,  1045,  2985,\n",
      "          2048,  2086,  1999, 19568,  2015,  1998,  2196,  3764,  2000,  2151,\n",
      "          1997,  2026, 10638,  1998,  2134,  1005,  1056,  2428,  3113, 10334,\n",
      "          1012,  2197,  2305,  1010,  2005,  1996,  2034,  2051,  1999,  2026,\n",
      "          2166,  1010,  1045,  2001,  4778,  2000,  1037,  2283,  1006,  2025,\n",
      "          7714,  2568,  2017,  1025,  1045,  2001,  1037,  2112,  1997,  1037,\n",
      "          2177,  2008,  2288, 13643,  4778,  1007,  1012,  1045, 10749,  1037,\n",
      "          2210,  2978,  1010,  2021,  2288, 10247,  2100,  2855,  1010,  1998,\n",
      "          2025,  4209,  2026,  6537,  1045,  3030,  1012,  1045,  2001,  3294,\n",
      "         17358,  2306,  1996,  2034,  3178,  1012,  4661, 25795,  1998,  2019,\n",
      "          8138, 14806,  1010,  1996,  2069,  2111,  1045,  2941,  3764,  2000,\n",
      "          2020,  1996,  2111,  1045,  2525,  2354,  1012,  1045,  2059,  2985,\n",
      "          1037,  2261,  2847,  3564,  1999,  1037,  3420,  1998, 14158,  7144,\n",
      "         27440,  1012,  2009,  2001,  3492,  6881,  1010, 14158,  2048,  2111,\n",
      "          2175,  2013,  3143, 12358,  1010,  2000,  2108,  1999,  2028,  2178,\n",
      "          1005,  1055,  2608,  2012,  1996,  2203,  1997,  1996,  2305,  1012,\n",
      "          3504,  2061,  3733,  1012,  2003,  2023,  1996,  4485,  2008,  1045,\n",
      "          1005,  2310,  2042,  1000,  4394,  1000,  2005,  2035,  1997,  2026,\n",
      "          2166,  1029,  1999,  2029,  2553,  1010, 10166,  1012,  2204,  9436,\n",
      "         25514,  1010,  1045,  1005,  2222,  2202,  2026,  6245,  2058,  2008,\n",
      "          2151,  2154,  1012,  3374,  2055,  1996,  2743,  2102,  1010,  1045,\n",
      "          3984,  2008,  1005,  1055,  2035,  2009,  2428,  2003,  1012,  1045,\n",
      "          2507,  2039,  1012,   102,  2028,  2518,  1045,  2572,  2183,  2000,\n",
      "          4012,  3549,  2094,  2017,  2006,  2025, 11664,  2135,  5948,  1998,\n",
      "          4760, 19355,  1012,  5948,  2003,  2788,  3378,  2007,  2591,  3450,\n",
      "          2021,  1996,  3119,  7245,  1997,  2108,  1037,  2990, 12054,  2003,\n",
      "          2025,  4276,  2009,  1012,   102]]), tensor([[101,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0, 102,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0, 102]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "print(construct_input_ref_pair(post, comment, ref_token_id, sep_token_id, cls_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "90cc14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, token_type_ids = construct_input_ref_pair(post, comment, ref_token_id, sep_token_id, cls_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "53dbeaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = construct_attention_mask(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "cd15b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a9c4a926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "3c3818c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4208e9",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "55377068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.4980, -4.1010, -5.0655]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = predict(input_ids,\n",
    "               token_type_ids=token_type_ids,\n",
    "               attention_mask=attention_mask)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f184c256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.4980, -4.1010, -5.0655]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = classification_forward_func(input_ids,\n",
    "               token_type_ids=token_type_ids,\n",
    "               attention_mask=attention_mask)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c75f69",
   "metadata": {},
   "source": [
    "## Apply LayerIntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "03d74c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(classification_forward_func, model.bert.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "d2df4439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<captum.attr._core.layer.layer_integrated_gradients.LayerIntegratedGradients at 0x7f73439c2a30>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "73346379",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribution, attribution_delta = lig.attribute(inputs=input_ids, \n",
    "                                               baselines=ref_input_ids,\n",
    "                                               target=2,\n",
    "                                               additional_forward_args=(token_type_ids, attention_mask),\n",
    "                                               return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "664afd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00],\n",
       "        [ 1.4611e-04,  1.3571e-03,  9.8245e-04,  ...,  5.2328e-05,\n",
       "          1.9038e-03,  1.2555e-03],\n",
       "        [-4.8884e-04, -6.8524e-04,  5.1778e-04,  ...,  2.0780e-04,\n",
       "          1.9909e-04, -1.5710e-05],\n",
       "        ...,\n",
       "        [-1.3424e-03,  2.6367e-04, -1.6980e-05,  ...,  3.6467e-05,\n",
       "         -1.5894e-03,  1.4697e-03],\n",
       "        [-1.3190e-03, -6.4167e-06,  2.8747e-04,  ..., -2.0616e-04,\n",
       "          2.0345e-04,  4.1385e-05],\n",
       "        [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "         -0.0000e+00, -0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "10a3af3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1639], dtype=torch.float64)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution_delta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
